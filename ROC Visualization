import numpy as np
from sklearn.metrics import roc_curve,precision_recall_curve,auc
from scipy import interp


mean_tpr = 0.0
mean_fpr = np.linspace(0,1,100)
all_tpr = []
precision,recall,average_precision = [],[],[]

for i,(train,test) in enumerate(cv.split(X,y)):
    clf = classifier.fit(X[train],y[train]).predict_proba(X[test])
    fpr,tpr,thresholds = roc_curve(y[test],clf[:,1])
    #mean_tpr += interp(mean_fpr,fpr,tpr)
    #mean_tpr[0] = 0.0
    roc_auc = auc(fpr,tpr)
    mean_tpr += interp(mean_fpr,fpr,tpr)

    mean_tpr[0] = 0
    
    plt.plot(fpr,tpr,lw = 1,label = 'ROC fold %{} (area = {:.2f})'.format(i,roc_auc))
    
    precision_score, recall_score, _ = precision_recall_curve(y[test], clf[:, 1])
    
    precision.append(precision_score)
    
    recall.append(recall_score)
    
    average_precision.append( average_precision_score(y[test], clf[:, 1]) )
    
plt.plot([0, 1], [0, 1], '--', color=(0.6, 0.6, 0.6), label='Luck')
    

mean_tpr /= 5
mean_tpr[-1] = 1.0
mean_auc = auc(mean_fpr, mean_tpr)
plt.plot(mean_fpr, mean_tpr, 'k--',
         label='Mean ROC (area = %0.2f)' % mean_auc, lw=2)


plt.xlim([-0.05, 1.05])
plt.ylim([-0.05, 1.05])
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('Receiver operating characteristic example')
plt.legend(loc="lower right")
plt.show()


## draw precison-call curve
plt.clf()
for i in range(folds):
    plt.plot(recall[i], precision[i],
             label='Precision-recall curve of fold {0} (area = {1:0.2f})'
                   ''.format(i+1, average_precision[i]))

plt.xlim([0.0, 1.0])
plt.ylim([0.0, 1.05])
plt.xlabel('Recall')
plt.ylabel('Precision')
plt.title('Extension of Precision-Recall curve to cross validation')
plt.legend(loc="lower right")
plt.show()

from sklearn.model_selection import learning_curve

def plot_learning_curve(estimator,title,X,y,ylim = None,cv = None,n_jobs = 1,train_sizes = np.linspace(0.1,1,5)):
    plt.figure()
    plt.title(title)
    if ylim is not None:
        plt.ylim(*ylim)

    train_sizes, train_scores, test_scores = learning_curve(
        estimator, X, y, cv=cv, n_jobs=n_jobs, train_sizes=train_sizes)
    train_scores_mean = np.mean(train_scores, axis=1)
    train_scores_std = np.std(train_scores, axis=1)
    test_scores_mean = np.mean(test_scores, axis=1)
    test_scores_std = np.std(test_scores, axis=1)
    plt.grid()
    
    plt.fill_between(train_sizes, train_scores_mean - train_scores_std,
                     train_scores_mean + train_scores_std, alpha=0.1,
                     color="r")
    plt.fill_between(train_sizes, test_scores_mean - test_scores_std,
                     test_scores_mean + test_scores_std, alpha=0.1, color="g")
    plt.plot(train_sizes, train_scores_mean, 'o-', color="r",
             label="Training score")
    plt.plot(train_sizes, test_scores_mean, 'o-', color="g",
             label="Cross-validation score")
    
    plt.legend(loc="best")
    return plt


# draw learning curve
digits = datasets.load_digits()
X, y = digits.data, digits.target

from sklearn.naive_bayes import GaussianNB

title = "Learning Curves (Naive Bayes)"
# Cross validation with 100 iterations to get smoother mean test and train
# score curves, each time with 20% data randomly selected as a validation set.

cv = StratifiedKFold(shuffle = True,n_splits = folds,random_state = 80)


estimator = GaussianNB()
plot_learning_curve(estimator, title, X, y, ylim=(0.7, 1.01), cv=cv, n_jobs=4)
plt.show()
